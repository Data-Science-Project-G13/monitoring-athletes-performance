{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xgboost\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import (GradientBoostingRegressor, GradientBoostingClassifier)\n",
    "pd.set_option('max.columns',100)\n",
    "pd.set_option('max.rows',500)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "matplotlib.rcParams['figure.figsize'] = (12,8)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from IPython.display import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "sns.set(style=\"darkgrid\", palette=\"pastel\", color_codes=True)\n",
    "sns.set_context('talk')\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import missingno as msno\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data \n",
    "os.listdir('/Users/sindhuveluguleti/Desktop/Semester -2 /Project/data files of project/data sets old ')\n",
    "#os.chdir(\"D:\\\\DSP2\\\\Git\\\\monitoring-athletes-performance\\\\main\")\n",
    "#data_path = '{}/data'.format(os.path.pardir)\n",
    "#athlete_csv_file = '{}/{}'.format(data_path, 'Eduardo Oliveira (Intermediate).csv')\n",
    "#print(athlete_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading eddy data and print its shape and data type\n",
    "#eddy=pd.read_csv(athlete_csv_file)\n",
    "selfdataframe=pd.read_csv('/Users/sindhuveluguleti/Desktop/Semester -2 /Project/data files of project/data sets old /Eddy.csv')\n",
    "print('eddy data shape: ', selfdataframe.shape)#shape \n",
    "print(selfdataframe.dtypes)#data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selfdataframe.drop(['Favorite', 'Aerobic TE', 'Avg Run Cadence', 'Max Run Cadence', 'Avg Stride Length', 'Avg Vertical Ratio', 'Avg Vertical Oscillation', 'Avg Ground Contact Time'\n",
    ",'Avg GCT Balance','L/R Balance','Grit','Flow','Total Reps','Total Sets','Bottom Time','Min Temp','Surface Interval','Decompression','Best Lap Time','Max Temp'], axis =1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selfdataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe.columns\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe.columns= selfdataframe.columns.str.replace(',', '')\n",
    "print(selfdataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe.head(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddy = selfdataframe.replace({\"--\": np.nan, \"...\": np.nan})#missing values replaced by nan\n",
    "#data_name[‘column_name’].replace(0, np.nan, inplace= True)\n",
    "eddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capitalization\n",
    "selfdataframe['Activity Type'] = selfdataframe['Activity Type'].str.lower()\n",
    "selfdataframe['Title'] = selfdataframe['Title'].str.lower()\n",
    "print(selfdataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formats\n",
    "selfdataframe['Elev Gain'] = selfdataframe['Elev Gain'].str.replace(',', '')\n",
    "selfdataframe['Elev Gain'] = selfdataframe['Elev Gain'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe[\"Elev Gain\"] = pd.to_numeric(selfdataframe[\"Elev Gain\"])\n",
    "print(selfdataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe['Elev Loss'] = selfdataframe['Elev Loss'].str.replace(',', '')\n",
    "selfdataframe['Elev Loss'] = selfdataframe['Elev Loss'].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe['Elev Loss'] = pd.to_numeric(selfdataframe['Elev Loss'])\n",
    "print(selfdataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe['Distance'] = selfdataframe['Distance'].str.replace(',', '')\n",
    "selfdataframe['Distance'] = selfdataframe['Distance'].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe['Distance'] = pd.to_numeric(selfdataframe['Distance'])\n",
    "print(selfdataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe['Calories'] = selfdataframe['Calories'].str.replace(',', '')\n",
    "selfdataframe['Calories'] = selfdataframe['Calories'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe['Calories'] = pd.to_numeric(selfdataframe['Calories'])\n",
    "print(selfdataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selfdataframe['Max Power'] = selfdataframe['Max Power'].str.replace(',', '')\n",
    "selfdataframe['Max Power'] = selfdataframe['Max Power'].astype(float)\n",
    "selfdataframe['Max Power'] = pd.to_numeric(selfdataframe['Max Power'])\n",
    "print(selfdataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe['Avg Power'] = selfdataframe['Avg Power'].astype(str)\n",
    "selfdataframe['Avg Power'] = selfdataframe['Avg Power'].str.replace(',', '')\n",
    "selfdataframe['Avg Power'] = selfdataframe['Avg Power'].astype(float)\n",
    "selfdataframe['Avg Power'] = pd.to_numeric(selfdataframe['Avg Power'])\n",
    "print(selfdataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe.loc[selfdataframe['Avg Speed'].str.contains(\":\", na=False), 'Avg Speed']=np.nan\n",
    "selfdataframe['Avg Speed'] = pd.to_numeric(selfdataframe['Avg Speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe.loc[selfdataframe['Max Speed'].str.contains(\":\", na=False), 'Max Speed']=np.nan\n",
    "selfdataframe['Max Speed'] = pd.to_numeric(selfdataframe['Max Speed'])\n",
    "print(selfdataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe[['Max Avg Power (20 min)', 'Avg Power', 'Avg Stroke Rate', 'Avg HR', 'Max HR', 'Total Strokes', 'Avg. Swolf', 'Avg Bike Cadence', 'Max Bike Cadence', 'Normalized Power® (NP®)', 'Number of Laps']] = selfdataframe[['Max Avg Power (20 min)', 'Avg Power', 'Avg Stroke Rate', 'Avg HR', 'Max HR', 'Total Strokes', 'Avg. Swolf', 'Avg Bike Cadence', 'Max Bike Cadence', 'Normalized Power® (NP®)' , 'Number of Laps']].apply(pd.to_numeric)\n",
    "print(selfdataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eddy['Date'] = pd.to_datetime(eddy['Date'], errors='coerce')\n",
    "#eddy[\"Date\"]= eddy[\"Date\"].dt.strftime(\"%d/%m/%Y %H:%M:%S \")\n",
    "\n",
    "#eddy['Date']=pd.to_datetime(eddy['Date'])\n",
    "selfdataframe['Date_extracted']=pd.to_datetime(selfdataframe[\"Date\"]).dt.normalize()\n",
    "#eddy['Time_extracted']=pd.to_timedelta(pd.to_datetime(eddy[\"Date\"]).dt.strftime('%H:%M:%S'))\n",
    "selfdataframe['Time_extracted']=pd.to_datetime(selfdataframe[\"Date\"]).dt.time\n",
    "selfdataframe['Date']=pd.to_datetime(selfdataframe['Date'])\n",
    "#print(pd.to_datetime(eddy['Date']).astype(datetime))\n",
    "#print(pd.to_datetime(eddy[\"Date\"]).dt.to_pydatetime())\n",
    "#print(pd.to_datetime(eddy[\"Date\"]).dt.normalize(),pd.to_datetime(eddy[\"Date\"]).dt.time,pd.to_timedelta(pd.to_datetime(eddy[\"Date\"]).dt.strftime('%H:%M:%S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import datetime\n",
    "#start = datetime.datetime.now()\n",
    "#end = datetime.datetime.now()\n",
    "#end - start\n",
    "#(end - start).total_seconds()\n",
    "\n",
    "#print(pd.to_timedelta(pd.to_datetime(eddy[\"Time\"]).dt.strftime('%H:%M:%S')),datetime.time.min)\n",
    "#eddy['Time']=pd.to_timedelta(pd.to_datetime(eddy[\"Time\"]).dt.strftime('%H:%M:%S'))\n",
    "#print(pd.to_timedelta(pd.to_datetime(eddy[\"Time\"]).dt.strftime('%H:%M:%S')).astype('datetime64[D]'))\n",
    "#print(eddy['Time']-datetime.time.min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfdataframe['Time_sec']=pd.to_timedelta(pd.to_datetime(selfdataframe[\"Time\"]).dt.strftime('%H:%M:%S')).dt.total_seconds()\n",
    "#eddy[\"Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selfdataframe.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling irregular data\n",
    "# select numeric columns\n",
    "def func_numeric():\n",
    "    eddy_numeric = selfdataframe.select_dtypes(include=[np.number])\n",
    "    numeric_cols = eddy_numeric.columns.values\n",
    "    return numeric_cols,eddy_numeric\n",
    "numeric_cols,eddy_numeric = func_numeric()\n",
    "print(numeric_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_categoric():\n",
    "    eddy_categoric= selfdataframe.select_dtypes(exclude=[np.number])\n",
    "    categoric_cols = eddy_categoric.columns.values\n",
    "    return  eddy_categoric,categoric_cols\n",
    "eddy_categoric,categoric_cols = func_categoric()\n",
    "print(categoric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_percent(data):\n",
    "    \"\"\"\n",
    "    Returns dataframe containing the total missing values and percentage of total\n",
    "    missing values of a column.\n",
    "    \"\"\"\n",
    "    miss_eddy = pd.DataFrame({'ColumnName':[],'TotalMissingVals':[],'PercentMissing':[]})\n",
    "    for col in data.columns:\n",
    "        sum_miss_val = data[col].isnull().sum()\n",
    "        percent_miss_val = round((sum_miss_val/data.shape[0])*100,2)\n",
    "        miss_eddy = miss_eddy.append(dict(zip(miss_eddy.columns,[col,sum_miss_val,percent_miss_val])),ignore_index=True)\n",
    "    return miss_eddy\n",
    "\n",
    "miss_eddy = find_missing_percent(selfdataframe)\n",
    "'''Columns with missing values'''\n",
    "print(f\"Number of columns with missing values: {str(miss_eddy[miss_eddy['PercentMissing']>0.0].shape[0])}\")\n",
    "display(miss_eddy[miss_eddy['PercentMissing']>0.0])\n",
    "#'''Drop the columns with more than 90% of missing values'''\n",
    "#drop_cols = miss_df[miss_df['PercentMissing'] >90.0].ColumnName.tolist()\n",
    "#eddy = eddy.drop(drop_cols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[51]:\n",
    "\n",
    "def missingno_bar():\n",
    "    graph = msno.bar(selfdataframe)\n",
    "    return graph\n",
    "print(missingno_bar())\n",
    "#msno.bar(eddy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingno_matrix():\n",
    "    matrix = msno.matrix(selfdataframe)\n",
    "    return matrix\n",
    "print(missingno_matrix())\n",
    "#msno.matrix(eddy)#for visulaising the locations of the missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_map():\n",
    "    heatmap=msno.heatmap(selfdataframe)\n",
    "    return(heatmap)\n",
    "print(heat_map())\n",
    "#msno.heatmap(eddy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deno_gram():\n",
    "    dendogram = msno.dendrogram(selfdataframe)\n",
    "    return(dendogram)\n",
    "print(deno_gram())\n",
    "#msno.dendrogram(eddy)# for grouping highly corelated   variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pandas_profiling.ProfileReport(selfdataframe)#not working with function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean imputation\n",
    "\n",
    "def mean_imputation(eddy_numeric2):\n",
    "    for col in eddy_numeric2.columns:\n",
    "        mean = eddy_numeric2[col].mean()\n",
    "        eddy_numeric2[col] = eddy_numeric2[col].fillna(mean)\n",
    "    return eddy_numeric2\n",
    "\n",
    "#eddy_numeric,numeric_cols=func_numeric()\n",
    "eddy_numeric2=selfdataframe[numeric_cols]\n",
    "eddy_mean_imp = mean_imputation(eddy_numeric2)\n",
    "eddy_mean_imp.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression imputaion\n",
    "'''Select all the numeric columns for regression imputation'''\n",
    "eddy_numeric_regr = selfdataframe[numeric_cols]\n",
    "'''Numeric columns with missing values which acts as target in training'''\n",
    "target_cols = ['Distance','Calories','Avg HR','Max HR','Elev Gain','Elev Loss','Avg Bike Cadence']\n",
    "'''Predictors for regression imputation'''\n",
    "predictors = eddy_numeric_regr.drop(target_cols, axis =1)\n",
    "\n",
    "def find_missing_index(eddy_numeric_regr, target_cols):\n",
    "  \n",
    "    miss_index_dict = {}\n",
    "    for tcol in target_cols:\n",
    "        index = eddy_numeric_regr[tcol][eddy_numeric_regr[tcol].isnull()].index\n",
    "        miss_index_dict[tcol] = index\n",
    "    return miss_index_dict\n",
    "\n",
    "\n",
    "def regression_imputation(eddy_numeric_regr, target_cols, miss_index_dict):\n",
    "    \n",
    "    for tcol in target_cols:\n",
    "        y = eddy_numeric_regr[tcol]\n",
    "        '''Initially impute the column with mean'''\n",
    "        y = y.fillna(y.mean())\n",
    "        xgb = xgboost.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "        '''Fit the model where y is the target column which is to be imputed'''\n",
    "        xgb.fit(predictors, y)\n",
    "        predictions = pd.Series(xgb.predict(predictors),index= y.index)    \n",
    "        index = miss_index_dict[tcol]\n",
    "        '''Replace the missing values with the predictions'''\n",
    "        eddy_numeric_regr[tcol].loc[index] = predictions.loc[index]\n",
    "    return eddy_numeric_regr\n",
    "\n",
    "miss_index_dict = find_missing_index(eddy_numeric_regr, target_cols)\n",
    "eddy_numeric_regr = regression_imputation(eddy_numeric_regr, target_cols, miss_index_dict)\n",
    "eddy_numeric_regr.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_imputation(eddy_categoric):\n",
    "    \"\"\"\n",
    "    Mode Imputation\n",
    "    \"\"\"\n",
    "    for col in eddy_categoric.columns:\n",
    "        mode = eddy_categoric[col].mode().iloc[0]\n",
    "        eddy_categoric[col] = eddy_categoric[col].fillna(mode)\n",
    "    return eddy_categoric\n",
    "\n",
    "eddy_mode_imp = mode_imputation(eddy_categoric)\n",
    "'''Concatenate the mean and mode imputed columns'''\n",
    "#eddy_imputed = pd.concat([eddy_mean_imp, eddy_mode_imp], axis = 1)\n",
    "#eddy_imputed.head()\n",
    "eddy_categoric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mice_imputation_numeric(eddy_numeric):\n",
    "    iter_imp_numeric = IterativeImputer(GradientBoostingRegressor())\n",
    "    imputed_eddy = iter_imp_numeric.fit_transform(eddy_numeric)\n",
    "    eddy_numeric_imp = pd.DataFrame(imputed_eddy, columns = eddy_numeric.columns, index= eddy_numeric.index)\n",
    "    return eddy_numeric_imp\n",
    "eddy_numeric_imp  = mice_imputation_numeric(eddy_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddy_numeric_imp.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mice_imputation_categoric(eddy_categoric):\n",
    "    ordinal_dict={}\n",
    "    for col in eddy_categoric:\n",
    "        ordinal_dict[col] = OrdinalEncoder()\n",
    "        nn_vals = np.array(eddy_categoric[col][eddy_categoric[col].notnull()]).reshape(-1,1)\n",
    "        nn_vals_arr = np.array(ordinal_dict[col].fit_transform(nn_vals)).reshape(-1,)\n",
    "        eddy_categoric[col].loc[eddy_categoric[col].notnull()] = nn_vals_arr\n",
    "    '''Impute the data using MICE with Gradient Boosting Classifier'''\n",
    "    iter_imp_categoric = IterativeImputer(GradientBoostingClassifier(), max_iter =5, initial_strategy='most_frequent')\n",
    "    imputed_eddy = iter_imp_categoric.fit_transform(eddy_categoric)\n",
    "    eddy_categoric_imp = pd.DataFrame(imputed_eddy, columns =eddy_categoric.columns,index = eddy_categoric.index).astype(int)\n",
    "    '''Inverse Transform'''\n",
    "    for col in eddy_categoric_imp.columns:\n",
    "        oe = ordinal_dict[col]\n",
    "        eddy_arr= np.array(eddy_categoric_imp[col]).reshape(-1,1)\n",
    "        eddy_categoric_imp[col] = oe.inverse_transform(eddy_arr)\n",
    "    return eddy_categoric_imp\n",
    "#eddy_categoric_imp = mice_imputation_categoric(eddy_categoric)\n",
    "\n",
    "#'''Concatenate Numeric and Categoric Training and Test set data '''\n",
    "#eddy_mice_imp = pd.join([eddy_numeric_imp, eddy_categoric_imp], axis = 1)\n",
    "#eddy_mice_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_interpolation(eddy_numeric):\n",
    "    for col in eddy_numeric.columns:\n",
    "        numeric = eddy_numeric.interpolate(method='linear', limit_direction='forward', axis=0).ffill().bfill()\n",
    "    return(numeric)\n",
    "eddy_Linearinterpolation = Linear_interpolation(eddy_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddy_Linearinterpolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eddy=eddy_numeric\n",
    "#eddy\n",
    "eddy_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler#when imputing a knn data must be normalised to reduce the bias in the imoutation\n",
    "scaler = MinMaxScaler()\n",
    "scaling = pd.DataFrame(scaler.fit_transform(eddy_numeric), columns = numeric_cols)\n",
    "selfdataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_imputation():\n",
    "    imputer = KNNImputer(n_neighbors = 23)\n",
    "    imputed_KNN = pd.DataFrame(imputer.fit_transform(eddy_numeric),columns = numeric_cols)\n",
    "    return imputed_KNN\n",
    "knn_imputation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eddy = knn_imputation()\n",
    "eddy.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "selfdataframe.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4,8))\n",
    "sns.boxplot(y = selfdataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_iqr(eddy , column):\n",
    "    global lower,upper\n",
    "    q25, q75 = np.quantile(eddy[column], 0.25), np.quantile(eddy[column], 0.75)\n",
    "    # calculate the IQR\n",
    "    iqr = q75 - q25\n",
    "    # calculate the outlier cutoff\n",
    "    cut_off = iqr * 1.5\n",
    "    # calculate the lower and upper bound value\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    print('The IQR is',iqr)\n",
    "    print('The lower bound value is', lower)\n",
    "    print('The upper bound value is', upper)\n",
    "    # Calculate the number of records below and above lower and above bound value respectively\n",
    "    df1 = eddy[eddy[column] > upper]\n",
    "    df2 = eddy[eddy[column] < lower]\n",
    "    return print('Total number of outliers are', df1.shape[0]+ df2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_iqr(selfdataframe, 'Distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "sns.distplot(selfdataframe.Distance, kde=False)\n",
    "plt.axvspan(xmin = lower, xmax= selfdataframe.Distance.min(), alpha=0.2, color='red')\n",
    "plt.axvspan(xmin = upper, xmax= selfdataframe.Distance.max(), alpha=0.2, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Frame without outliers\n",
    "#df_new = eddy[(eddy['Distance'] < upper) | (eddy['Distance'] > lower)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.distplot(selfdataframe['Max Power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_std(eddy, column):\n",
    "    global lower,upper\n",
    "    # calculate the mean and standard deviation of the data frame\n",
    "    data_mean, data_std = eddy[column].mean(), eddy[column].std()\n",
    "    # calculate the cutoff value\n",
    "    cut_off = data_std * 3\n",
    "    # calculate the lower and upper bound value\n",
    "    lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "    print('The lower bound value is', lower)\n",
    "    print('The upper bound value is', upper)\n",
    "    # Calculate the number of records below and above lower and above bound value respectively\n",
    "    df1 = eddy[eddy[column] > upper]\n",
    "    df2 = eddy[eddy[column] < lower]\n",
    "    return print('Total number of outliers are', df1.shape[0]+ df2.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_std(selfdataframe, 'Max Power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.distplot(selfdataframe['Max Power'], kde=False)\n",
    "plt.axvspan(xmin = lower, xmax= selfdataframe['Max Power'].min(), alpha=0.2, color='red')\n",
    "plt.axvspan(xmin = upper, xmax= selfdataframe['Max Power'].max(), alpha=0.2, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Frame without outliers\n",
    "#df_new = eddy[(eddy['Max Power'] < upper) | (eddy['Max Power'] > lower)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zscore\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.distplot(selfdataframe['Elev Gain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_zscore(eddy):\n",
    "    global outliers,zscore\n",
    "    outliers = []\n",
    "    zscore = []\n",
    "    threshold = 3\n",
    "    mean = np.mean(eddy)\n",
    "    std = np.std(eddy)\n",
    "    for i in eddy:\n",
    "        z_score= (i - mean)/std \n",
    "        zscore.append(z_score)\n",
    "        if np.abs(z_score) > threshold:\n",
    "            outliers.append(i)\n",
    "    return print(\"Total number of outliers are\",len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_zscore(selfdataframe['Elev Gain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "sns.distplot(zscore)\n",
    "plt.axvspan(xmin = 3 ,xmax= max(zscore),alpha=0.2, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new = eddy[(eddy['Calories'] < 3) | (eddy['Calories'] > -3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if\n",
    "#Import necessary libraries\n",
    "from sklearn.ensemble import IsolationForest\n",
    "#The required columns\n",
    "cols = ['Distance','Avg HR', 'Max HR']\n",
    "#Plotting the sub plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5), facecolor='w', edgecolor='k')\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, column in enumerate(cols):\n",
    "    isolation_forest = IsolationForest(contamination='auto')\n",
    "    isolation_forest.fit(selfdataframe[column].values.reshape(-1, 1))\n",
    "\n",
    "    xx = np.linspace(selfdataframe[column].min(), selfdataframe[column].max(), len(selfdataframe)).reshape(-1, 1)\n",
    "    anomaly_score = isolation_forest.decision_function(xx)\n",
    "    outlier = isolation_forest.predict(xx)\n",
    "    \n",
    "    axs[i].plot(xx, anomaly_score, label='anomaly score')\n",
    "    axs[i].fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), \n",
    "                     where=outlier==-1, color='r', \n",
    "                     alpha=.4, label='outlier region')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB scan\n",
    "X = selfdataframe[['Distance', 'Max HR']].values\n",
    "\n",
    "db = DBSCAN(eps=3.0, min_samples=10).fit(X)\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(labels).value_counts()#-1 represents outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))#red outliers\n",
    "\n",
    "unique_labels = set(labels)\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "for color,label in zip(colors, unique_labels):\n",
    "    sample_mask = [True if l == label else False for l in labels]\n",
    "    plt.plot(X[:,0][sample_mask], X[:, 1][sample_mask], 'o', color=color);\n",
    "plt.xlabel('Distance');\n",
    "plt.ylabel('Max HR');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lofLocal Outlier Factor Method\n",
    "clf = LocalOutlierFactor(n_neighbors=50, contamination='auto')\n",
    "X = selfdataframe[['Avg Speed', 'Max Speed']].values\n",
    "y_pred = clf.fit_predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))#red outliers ,blue nrml records\n",
    "# plot the level sets of the decision function\n",
    "\n",
    "in_mask = [True if l == 1 else False for l in y_pred]\n",
    "out_mask = [True if l == -1 else False for l in y_pred]\n",
    "\n",
    "plt.title(\"Local Outlier Factor (LOF)\")\n",
    "# inliers\n",
    "a = plt.scatter(X[in_mask, 0], X[in_mask, 1], c = 'blue',\n",
    "                edgecolor = 'k', s = 30)\n",
    "# outliers\n",
    "b = plt.scatter(X[out_mask, 0], X[out_mask, 1], c = 'red',\n",
    "                edgecolor = 'k', s = 30)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Avg Speed');\n",
    "plt.ylabel('Max Speed');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
